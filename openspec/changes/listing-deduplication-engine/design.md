## Context

- 同一實體房源在 591 常由不同房仲重複刊登，`listing_id` 不同但物件本體相同，導致資料庫與通知重複。
- 目前系統主要以 `(source, listing_id)` 與 `raw_hash` 控制重複，但 `raw_hash` 對標題與地址格式變化過度敏感，無法穩定判定跨房仲同屋。
- 資料庫已有既存重複資料，且已和 `notifications_sent`、`listings_read`、`favorites` 建立關聯，清理時必須保留關聯一致性。

## Goals / Non-Goals

**Goals:**
- 在爬蟲寫入前以可解釋規則判定「同一實體房源」，重複時略過寫入。
- 提供既有資料批次去重工具，將重複群組合併為單一主記錄。
- 合併過程保留通知、已讀、最愛等關聯狀態，不破壞 bot 體驗。
- 產出可追蹤統計與日誌，便於校正演算法門檻。

**Non-Goals:**
- 不嘗試跨來源（非 591）的多平台去重。
- 不做 ML 模型訓練；先以規則化特徵 + 分數門檻落地。
- 不改變使用者設定流程（/settings）與既有篩選條件。

## Decisions

- **主鍵策略：導入實體指紋欄位**  
  為 listings 新增 `entity_fingerprint`（可索引），由地址正規化、行政區、坪數區間、價格區間、格局、樓層等特徵組合。  
  - 為何：比單純 `raw_hash` 更穩定，可容忍標題文案差異。  
  - 替代方案：直接沿用 `raw_hash`。缺點是對標題字詞微變高度敏感，漏判嚴重。

- **判定策略：兩段式去重**  
  第一段用快速 fingerprint 命中候選；第二段計算相似度分數（地址 token 相似、價格/坪數差距、格局一致）再做最終判定。  
  - 為何：兼顧效能與準確率。  
  - 替代方案：僅 fingerprint 完全相等。缺點是對真實資料噪音容忍不足。

- **寫入策略：在線去重 + 本輪去重**  
  每次 scrape 寫入時同時比對 DB 既有資料與本輪暫存集合，避免同輪與跨輪重複插入。  
  - 為何：可立即降低重複資料增長速度。  
  - 替代方案：僅離線批次清理。缺點是重複資料仍持續產生。

- **清理策略：選主記錄合併關聯**  
  每個重複群組保留一筆 canonical record（優先資料較完整、較新發布或已被關聯最多者），其餘標記刪除並轉移關聯到主記錄。  
  - 為何：避免直接硬刪造成 read/notified/favorite 狀態遺失。  
  - 替代方案：直接刪除重複。缺點是關聯資料斷裂，使用者體驗不一致。

- **可觀測性：記錄 decision trace**  
  對每次略過/合併記錄 fingerprint、分數、命中主因與候選 listing_id。  
  - 為何：便於調整門檻與分析誤判。

## Risks / Trade-offs

- **[Risk] 誤判把不同物件當同屋** → Mitigation：提高高風險欄位權重、設定最低置信分數、先 dry-run 報表再執行刪除。
- **[Risk] 漏判導致重複仍存在** → Mitigation：保留人工可調門檻與重跑清理能力，持續檢視統計。
- **[Risk] 清理耗時影響線上排程** → Mitigation：批次分段處理、限制單次處理量，離峰執行。
- **[Risk] 關聯轉移不完整** → Mitigation：在 transaction 內更新所有關聯表並加驗證查詢。

## Migration Plan

1. 新增 schema/migration：`entity_fingerprint`（與必要索引/輔助表）。
2. 上線「只計算不刪除」模式（dry-run）輸出重複群組統計。
3. 驗證誤判率後啟用在線去重（scrape insert path）。
4. 執行既有資料清理（分批 transaction），轉移關聯並刪除重複列。
5. 監控略過率、合併率與 bot 錯誤，必要時回退至僅記錄模式。

## Open Questions

- canonical record 優先規則是否要偏向「最完整欄位」或「最新發布時間」？
- 對價格/坪數差異容忍門檻（例如 3%/5%）要用全域固定值還是依區域分層？
- 清理工具要做成 bot command（管理者觸發）或 CLI maintenance command？
